# ğŸ¯ CartPole Reinforcement Learning

<div align="center">

![Python](https://img.shields.io/badge/Python-3.10%2B-blue.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)
![Reinforcement Learning](https://img.shields.io/badge/RL-PPO-orange.svg)
![Environment](https://img.shields.io/badge/Environment-CartPole-red.svg)

**A sophisticated reinforcement learning agent that masters the CartPole environment using the Proximal Policy Optimization (PPO) algorithm.**

*Experience the power of modern RL with fast, reproducible setups powered by `uv`.*

</div>

---

## ğŸ“š Table of Contents

- [ğŸ¯ Overview](#-overview)
- [âœ¨ Features](#-features)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ“¦ Installation](#-installation)
- [ğŸ® Usage](#-usage)
- [ğŸ“Š Monitoring](#-monitoring)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ”§ How It Works](#-how-it-works)
- [ğŸ“– Resources](#-resources)
- [ğŸ¤ Contributing](#-contributing)
- [ğŸ“„ License](#-license)

---

## ğŸ¯ Overview

CartPoleRL is a state-of-the-art reinforcement learning project that demonstrates the power of the **Proximal Policy Optimization (PPO)** algorithm in solving the classic CartPole balancing problem. The agent learns to balance a pole on a cart by making strategic left and right movements, showcasing fundamental RL concepts in an intuitive environment.

## âœ¨ Features

- ğŸ¯ **PPO Algorithm**: State-of-the-art policy optimization
- ğŸƒâ€â™‚ï¸ **Fast Setup**: One-command installation with `uv`
- ğŸ“Š **Real-time Monitoring**: TensorBoard integration for training visualization
- ğŸ”§ **GPU Support**: Optimized for NVIDIA GPU acceleration
- ğŸ“ˆ **Model Checkpoints**: Automatic saving and loading of trained models
- ğŸ® **Multiple Versions**: Support for different model configurations
- ğŸ”„ **Reproducible**: Locked dependencies for consistent results

---

## ğŸš€ Quick Start

Get up and running in under 2 minutes:

```bash
# Clone and navigate
git clone https://github.com/amugoodbad229/CartPoleRL.git
cd CartPoleRL

# Install everything
uv sync

# Activate environment
source .venv/bin/activate  # Linux/macOS
# or
.venv\Scripts\activate     # Windows

# Start training
python main-v1.py
```

---

## ğŸ“¦ Installation

### ğŸ”§ Prerequisites

Before you begin, ensure you have:

| Requirement | Version | Notes |
|------------|---------|-------|
| ğŸ Python | 3.10+ | Required for modern ML libraries |
| ğŸ® NVIDIA GPU | Latest drivers | For accelerated training |
| ğŸ”— ProtoTwin Connect | Latest | PTC integration |

### ğŸ“¥ Step-by-Step Installation

**1. Clone the Repository**
```bash
git clone https://github.com/amugoodbad229/CartPoleRL.git
cd CartPoleRL
```

**2. One-Command Setup** âš¡
```bash
uv sync
```
> This creates a local `.venv` and installs all dependencies, including the correct GPU version of PyTorch

**3. Activate the Environment**
```bash
# ğŸ§ Linux/macOS 
source .venv/bin/activate

# ğŸªŸ Windows
.venv\Scripts\activate
```

---

## ğŸ® Usage

### ğŸ‹ï¸ Training the Agent

Start training with your preferred model version:
```bash
python main-v1.py  # Version 1
python main-v2.py  # Version 2
# ... or any other version
```

---

## ğŸ“Š Monitoring

### ğŸ” TensorBoard Visualization

Monitor your agent's training progress in real-time:

**Option 1: Specific Model Monitoring**
```bash
# Navigate to specific PPO model directory
cd tensorboard/PPO_1    # or PPO_2, PPO_3, etc.

# Launch TensorBoard
tensorboard --logdir .
```

**Option 2: Version-Specific Monitoring**
```bash
# Launch TensorBoard for specific version
python -m tensorboard.main --logdir tensorboard-v1  # Replace v1 with your version
```

**Option 3: All Models Overview**
```bash
# Monitor all models simultaneously
tensorboard --logdir tensorboard/
```

> ğŸ’¡ **Tip**: Open your browser to `http://localhost:6006` to view the TensorBoard dashboard

---

## ğŸ“ Project Structure

```
CartPoleRL/
â”‚
â”œâ”€â”€ ğŸ—‚ï¸ .venv/                     # Virtual environment (managed by uv)
â”œâ”€â”€ ğŸ’¾ logs-v*/checkpoints/       # Saved model checkpoints
â”œâ”€â”€ ğŸ“Š tensorboard-v*/            # TensorBoard logs for training visualization
â”œâ”€â”€ ğŸš« .gitignore                 # Git ignore rules
â”œâ”€â”€ ğŸ”„ export_onnx.py             # Convert SB3 policy to ONNX format
â”œâ”€â”€ ğŸ¯ main-v*.py                 # Training scripts (multiple versions)
â”œâ”€â”€ ğŸ“‹ pyproject.toml             # Project dependencies and metadata
â”œâ”€â”€ ğŸ“– README.md                  # Project documentation
â””â”€â”€ ğŸ”’ uv.lock                    # Lockfile for reproducible builds
```

---

## ğŸ”§ How It Works

### ğŸ® Environment
The agent operates in the **CartPole-v1** environment from the Gymnasium library, where:
- **Goal**: Balance a pole on a moving cart
- **Actions**: Move cart left (0) or right (1)
- **Observation**: Cart position, cart velocity, pole angle, pole angular velocity
- **Reward**: +1 for each timestep the pole remains upright

### ğŸ§  Algorithm
**Proximal Policy Optimization (PPO)** - A state-of-the-art reinforcement learning algorithm that:
- âœ… Stable and reliable training
- âš¡ Sample efficient
- ğŸ¯ Prevents destructive policy updates
- ğŸ“š Implemented via Stable Baselines3

### ğŸ¯ Objective
Learn an optimal policy that maximizes the time the pole stays balanced by making strategic cart movements.

---

## ğŸ“– Resources

### ğŸ“„ Documentation & References
- **[ğŸ“‹ Project Guide (PDF)](https://jumpshare.com/share/5R2Vt26zIvwhY93lSeQS)** - Important concepts and understandings
- **[ğŸ¨ Resource Collection (tldraw)](https://www.tldraw.com/f/T6oHe2VW4S5P4fRhE0Aqv?d=v-941.3915.2132.1013.0Nu4aCQvq1Lg7bbzkZt0N)** - Visual resources and diagrams

### ğŸ› ï¸ Useful Git Commands

<details>
<summary>Click to expand Git reference</summary>

```bash
# Check current status
git status

# Stage all changes
git add .

# Commit with descriptive message
git commit -m "Add CartPole RL implementation with PPO and ProtoTwin integration"

# Push to remote repository
git push
```

</details>

---

## ğŸ¤ Contributing

We welcome contributions! Here's how you can help:

1. ğŸ´ Fork the repository
2. ğŸŒŸ Create a feature branch (`git checkout -b feature/amazing-feature`)
3. ğŸ’¾ Commit your changes (`git commit -m 'Add amazing feature'`)
4. ğŸ“¤ Push to the branch (`git push origin feature/amazing-feature`)
5. ğŸ”€ Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

<div align="center">

**Made with â¤ï¸ for the RL community**

â­ Star this repo if you found it helpful!

</div>
